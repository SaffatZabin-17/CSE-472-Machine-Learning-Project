{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10184793,"sourceType":"datasetVersion","datasetId":6291826}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setting up environment","metadata":{}},{"cell_type":"code","source":"!pip install timm==0.4.12\n!pip install fvcore einops submitit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:24:15.885688Z","iopub.execute_input":"2024-12-15T02:24:15.886485Z","iopub.status.idle":"2024-12-15T02:24:38.293479Z","shell.execute_reply.started":"2024-12-15T02:24:15.886444Z","shell.execute_reply":"2024-12-15T02:24:38.292591Z"}},"outputs":[{"name":"stdout","text":"Collecting timm==0.4.12\n  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.10/site-packages (from timm==0.4.12) (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm==0.4.12) (0.19.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12) (2024.6.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->timm==0.4.12) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm==0.4.12) (10.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4->timm==0.4.12) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4->timm==0.4.12) (1.3.0)\nDownloading timm-0.4.12-py3-none-any.whl (376 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: timm\n  Attempting uninstall: timm\n    Found existing installation: timm 1.0.11\n    Uninstalling timm-1.0.11:\n      Successfully uninstalled timm-1.0.11\nSuccessfully installed timm-0.4.12\nCollecting fvcore\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting submitit\n  Downloading submitit-1.5.2-py3-none-any.whl.metadata (7.9 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fvcore) (1.26.4)\nCollecting yacs>=0.1.6 (from fvcore)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from fvcore) (6.0.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fvcore) (4.66.4)\nRequirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.10/site-packages (from fvcore) (2.4.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from fvcore) (10.3.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from fvcore) (0.9.0)\nCollecting iopath>=0.1.7 (from fvcore)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.1 in /opt/conda/lib/python3.10/site-packages (from submitit) (3.1.0)\nRequirement already satisfied: typing_extensions>=3.7.4.2 in /opt/conda/lib/python3.10/site-packages (from submitit) (4.12.2)\nCollecting portalocker (from iopath>=0.1.7->fvcore)\n  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading submitit-1.5.2-py3-none-any.whl (74 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.9/74.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: fvcore, iopath\n  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=a12a039f1078db324b71a4ee08979f1ff9f3898625ee2e6b3d9cd3dfecc60463\n  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=36ae42edc189dc2eb25458db75a6d71f7419d787ae997581277c6c6d8ba4aba4\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built fvcore iopath\nInstalling collected packages: yacs, submitit, portalocker, einops, iopath, fvcore\nSuccessfully installed einops-0.8.0 fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.0.0 submitit-1.5.2 yacs-0.1.8\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Imports and Constants","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import precision_score, recall_score, roc_auc_score, average_precision_score, confusion_matrix\nimport pandas as pd\nimport gc\n\n# Paths to the dataset\ndata_dir = '/kaggle/input/brain-tumor-classification-mri'\ntrain_dir = os.path.join(data_dir, 'Training')\ntest_dir = os.path.join(data_dir, 'Testing')\n\n# Parameters\nnum_classes = 4\nbatch_size = 16\nnum_epochs = 20\nlearning_rate = 1e-4\nval_split = 0.2  # 20% of combined training data for validation\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:24:38.295763Z","iopub.execute_input":"2024-12-15T02:24:38.296181Z","iopub.status.idle":"2024-12-15T02:24:41.932363Z","shell.execute_reply.started":"2024-12-15T02:24:38.296137Z","shell.execute_reply":"2024-12-15T02:24:41.931667Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Dataset Preprocessing","metadata":{}},{"cell_type":"code","source":"# Normalization\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n# Data Transforms\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Add color jitter\n    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),  # Random affine transformations\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random zoom\n    transforms.ToTensor(),\n    normalize\n])\n\nval_test_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    normalize\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:24:41.933309Z","iopub.execute_input":"2024-12-15T02:24:41.933662Z","iopub.status.idle":"2024-12-15T02:24:41.939581Z","shell.execute_reply.started":"2024-12-15T02:24:41.933638Z","shell.execute_reply":"2024-12-15T02:24:41.938689Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Dataset Loading","metadata":{}},{"cell_type":"code","source":"# Load the datasets\nfull_train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n\n# Split the dataset into training and validation\ntrain_size = int((1 - val_split) * len(full_train_dataset))\nval_size = len(full_train_dataset) - train_size\ntrain_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n\n# Override transform for validation dataset\nval_dataset.dataset.transform = val_test_transform\n\n# Test dataset\ntest_dataset = datasets.ImageFolder(root=test_dir, transform=val_test_transform)\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:24:41.941158Z","iopub.execute_input":"2024-12-15T02:24:41.941410Z","iopub.status.idle":"2024-12-15T02:25:13.549336Z","shell.execute_reply.started":"2024-12-15T02:24:41.941386Z","shell.execute_reply":"2024-12-15T02:25:13.548621Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Cloning Cross-ViT git and Importing models","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/IBM/CrossViT.git\nimport sys\nsys.path.append(\"CrossViT\")\n\nfrom models.crossvit import crossvit_9_224, crossvit_15_224, crossvit_18_224, crossvit_9_dagger_224, crossvit_15_dagger_224, crossvit_15_dagger_384, crossvit_18_dagger_224, crossvit_18_dagger_384\n\n# Models to evaluate\nmodel_names = [\n    \"crossvit_9_224\",\n    \"crossvit_15_224\",\n    \"crossvit_18_224\",\n    \"crossvit_9_dagger_224\",\n    \"crossvit_15_dagger_224\",\n    \"crossvit_15_dagger_384\",\n    \"crossvit_18_dagger_224\",\n    \"crossvit_18_dagger_384\"\n]\n\nmodels = [\n    crossvit_9_224,\n    crossvit_15_224,\n    crossvit_18_224,\n    crossvit_9_dagger_224,\n    crossvit_15_dagger_224,\n    crossvit_15_dagger_384,\n    crossvit_18_dagger_224,\n    crossvit_18_dagger_384\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:25:13.550229Z","iopub.execute_input":"2024-12-15T02:25:13.550475Z","iopub.status.idle":"2024-12-15T02:25:16.279497Z","shell.execute_reply.started":"2024-12-15T02:25:13.550450Z","shell.execute_reply":"2024-12-15T02:25:16.278482Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'CrossViT'...\nremote: Enumerating objects: 64, done.\u001b[K\nremote: Counting objects: 100% (64/64), done.\u001b[K\nremote: Compressing objects: 100% (46/46), done.\u001b[K\nremote: Total 64 (delta 24), reused 43 (delta 13), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (64/64), 33.12 KiB | 8.28 MiB/s, done.\nResolving deltas: 100% (24/24), done.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Training and Evaluation function","metadata":{}},{"cell_type":"code","source":"# Metrics to store\nmetrics = []\n\n# Training and Evaluation Functions\ndef train_one_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    train_bar = tqdm(loader, desc=\"Training\", leave=False)\n    for inputs, labels in train_bar:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        _, preds = torch.max(outputs, 1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n        train_bar.set_postfix(loss=loss.item())\n\n    epoch_loss = running_loss / total\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc\n\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    all_labels = []\n    all_preds = []\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    all_labels = []\n    all_preds = []\n\n    val_bar = tqdm(loader, desc=\"Validation\", leave=False)\n    with torch.no_grad():\n        for inputs, labels in val_bar:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            running_loss += loss.item() * inputs.size(0)\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n\n    epoch_loss = running_loss / total\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc, all_labels, all_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:25:16.280795Z","iopub.execute_input":"2024-12-15T02:25:16.281732Z","iopub.status.idle":"2024-12-15T02:25:16.291435Z","shell.execute_reply.started":"2024-12-15T02:25:16.281692Z","shell.execute_reply":"2024-12-15T02:25:16.290733Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Training and Testing Loop","metadata":{}},{"cell_type":"code","source":"# Training Loop for Multiple Models\nensemble_predictions = []  # To store all models' predictions for ensemble\nensemble_labels = []  # Ground truth labels for comparison\nfor model_name, model_fn in zip(model_names, models):\n    torch.cuda.empty_cache()\n    print(f\"Evaluating {model_name}\")\n    model = model_fn(pretrained=False, num_classes=num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\n    train_losses, val_losses = [], []\n    best_val_acc = 0.0\n\n    for epoch in range(num_epochs):\n        scheduler.step()  # Adjust learning rate\n        print(f\"Epoch [{epoch+1}/{num_epochs}] for {model_name}\")\n        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n        val_loss, val_acc, val_labels, val_preds = evaluate(model, val_loader, criterion, device)\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n\n        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n        print(f\"  Val Loss:   {val_loss:.4f}, Val Acc:   {val_acc:.4f}\")\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), f\"best_{model_name}.pth\")\n            print(\"  [*] Saved best model.\")\n\n    # Test the Best Model\n    model.load_state_dict(torch.load(f\"best_{model_name}.pth\"))\n    test_loss, test_acc, test_labels, test_preds = evaluate(model, test_loader, criterion, device)\n\n    # Append model predictions for ensemble\n    ensemble_predictions.append(test_preds)\n    if not ensemble_labels:\n        ensemble_labels = test_labels  # Use ground truth labels from the first model\n\n    # Calculate Metrics\n    precision = precision_score(test_labels, test_preds, average='weighted')\n    recall = recall_score(test_labels, test_preds, average='weighted')\n    auroc = roc_auc_score(test_labels, nn.functional.one_hot(torch.tensor(test_preds), num_classes=num_classes), multi_class='ovr')\n    #aupr = average_precision_score(test_labels, nn.functional.one_hot(torch.tensor(test_preds), num_classes=num_classes), average='weighted')\n\n    # Confusion Matrix\n    cm = confusion_matrix(test_labels, test_preds)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(num_classes), yticklabels=range(num_classes))\n    plt.title(f\"Confusion Matrix for {model_name}\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.savefig(f\"confusion_matrix_{model_name}.png\")\n    plt.close()\n\n    # Store Metrics\n    metrics.append({\n        \"Model\": model_name,\n        \"Test Accuracy\": test_acc,\n        \"Precision\": precision,\n        \"Recall\": recall,\n        \"AUROC\": auroc,\n    })\n\n    # Plot Loss Curves\n    plt.figure()\n    plt.plot(range(num_epochs), train_losses, label=\"Training Loss\")\n    plt.plot(range(num_epochs), val_losses, label=\"Validation Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Loss Curves for {model_name}\")\n    plt.legend()\n    plt.savefig(f\"loss_curves_{model_name}.png\")\n    plt.close()\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:25:16.292861Z","iopub.execute_input":"2024-12-15T02:25:16.293217Z"}},"outputs":[{"name":"stdout","text":"Evaluating crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/20] for crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 1.0350, Train Acc: 0.5638\n  Val Loss:   0.9698, Val Acc:   0.6038\n  [*] Saved best model.\nEpoch [2/20] for crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.8349, Train Acc: 0.6779\n  Val Loss:   0.7853, Val Acc:   0.6867\n  [*] Saved best model.\nEpoch [3/20] for crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.6938, Train Acc: 0.7338\n  Val Loss:   0.6463, Val Acc:   0.7539\n  [*] Saved best model.\nEpoch [4/20] for crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.5769, Train Acc: 0.7838\n  Val Loss:   0.4981, Val Acc:   0.8166\n  [*] Saved best model.\nEpoch [5/20] for crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.3929, Train Acc: 0.8614\n  Val Loss:   0.4148, Val Acc:   0.8504\n  [*] Saved best model.\nEpoch [6/20] for crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.3350, Train Acc: 0.8864\n  Val Loss:   0.3828, Val Acc:   0.8565\n  [*] Saved best model.\nEpoch [7/20] for crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.3027, Train Acc: 0.8973\n  Val Loss:   0.3857, Val Acc:   0.8641\n  [*] Saved best model.\nEpoch [8/20] for crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2744, Train Acc: 0.9091\n  Val Loss:   0.3519, Val Acc:   0.8696\n  [*] Saved best model.\nEpoch [9/20] for crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2505, Train Acc: 0.9189\n  Val Loss:   0.3386, Val Acc:   0.8802\n  [*] Saved best model.\nEpoch [10/20] for crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2201, Train Acc: 0.9294\n  Val Loss:   0.3213, Val Acc:   0.8853\n  [*] Saved best model.\nEpoch [11/20] for crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2121, Train Acc: 0.9325\n  Val Loss:   0.3196, Val Acc:   0.8858\n  [*] Saved best model.\nEpoch [12/20] for crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2084, Train Acc: 0.9352\n  Val Loss:   0.3167, Val Acc:   0.8848\nEpoch [13/20] for crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2053, Train Acc: 0.9353\n  Val Loss:   0.3159, Val Acc:   0.8848\nEpoch [14/20] for crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2023, Train Acc: 0.9375\n  Val Loss:   0.3142, Val Acc:   0.8883\n  [*] Saved best model.\nEpoch [15/20] for crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1978, Train Acc: 0.9383\n  Val Loss:   0.3139, Val Acc:   0.8868\nEpoch [16/20] for crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1975, Train Acc: 0.9390\n  Val Loss:   0.3135, Val Acc:   0.8873\nEpoch [17/20] for crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1971, Train Acc: 0.9388\n  Val Loss:   0.3135, Val Acc:   0.8878\nEpoch [18/20] for crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1968, Train Acc: 0.9397\n  Val Loss:   0.3133, Val Acc:   0.8878\nEpoch [19/20] for crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1965, Train Acc: 0.9395\n  Val Loss:   0.3136, Val Acc:   0.8883\nEpoch [20/20] for crossvit_9_224\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_23/1389378619.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f\"best_{model_name}.pth\"))\n","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1960, Train Acc: 0.9393\n  Val Loss:   0.3135, Val Acc:   0.8888\n  [*] Saved best model.\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Evaluating crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/20] for crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 1.0137, Train Acc: 0.5790\n  Val Loss:   0.8691, Val Acc:   0.6468\n  [*] Saved best model.\nEpoch [2/20] for crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.8238, Train Acc: 0.6754\n  Val Loss:   0.8169, Val Acc:   0.7039\n  [*] Saved best model.\nEpoch [3/20] for crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.6538, Train Acc: 0.7435\n  Val Loss:   0.7414, Val Acc:   0.7034\nEpoch [4/20] for crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.5286, Train Acc: 0.8011\n  Val Loss:   0.5404, Val Acc:   0.7873\n  [*] Saved best model.\nEpoch [5/20] for crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.3523, Train Acc: 0.8707\n  Val Loss:   0.3848, Val Acc:   0.8580\n  [*] Saved best model.\nEpoch [6/20] for crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2976, Train Acc: 0.8958\n  Val Loss:   0.3520, Val Acc:   0.8752\n  [*] Saved best model.\nEpoch [7/20] for crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2597, Train Acc: 0.9080\n  Val Loss:   0.3361, Val Acc:   0.8807\n  [*] Saved best model.\nEpoch [8/20] for crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2356, Train Acc: 0.9174\n  Val Loss:   0.3094, Val Acc:   0.8919\n  [*] Saved best model.\nEpoch [9/20] for crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2048, Train Acc: 0.9301\n  Val Loss:   0.3192, Val Acc:   0.8858\nEpoch [10/20] for crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1745, Train Acc: 0.9434\n  Val Loss:   0.2962, Val Acc:   0.8964\n  [*] Saved best model.\nEpoch [11/20] for crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1687, Train Acc: 0.9486\n  Val Loss:   0.2927, Val Acc:   0.8989\n  [*] Saved best model.\nEpoch [12/20] for crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1647, Train Acc: 0.9491\n  Val Loss:   0.2901, Val Acc:   0.8999\n  [*] Saved best model.\nEpoch [13/20] for crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1608, Train Acc: 0.9505\n  Val Loss:   0.2904, Val Acc:   0.8989\nEpoch [14/20] for crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1581, Train Acc: 0.9511\n  Val Loss:   0.2893, Val Acc:   0.8984\nEpoch [15/20] for crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1535, Train Acc: 0.9535\n  Val Loss:   0.2884, Val Acc:   0.8999\nEpoch [16/20] for crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1531, Train Acc: 0.9538\n  Val Loss:   0.2879, Val Acc:   0.8999\nEpoch [17/20] for crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1528, Train Acc: 0.9544\n  Val Loss:   0.2885, Val Acc:   0.8994\nEpoch [18/20] for crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1525, Train Acc: 0.9544\n  Val Loss:   0.2879, Val Acc:   0.9005\n  [*] Saved best model.\nEpoch [19/20] for crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1522, Train Acc: 0.9540\n  Val Loss:   0.2882, Val Acc:   0.8994\nEpoch [20/20] for crossvit_15_224\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_23/1389378619.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f\"best_{model_name}.pth\"))\n","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1517, Train Acc: 0.9546\n  Val Loss:   0.2882, Val Acc:   0.8994\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Evaluating crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/20] for crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 1.0493, Train Acc: 0.5517\n  Val Loss:   0.9116, Val Acc:   0.6483\n  [*] Saved best model.\nEpoch [2/20] for crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.8834, Train Acc: 0.6520\n  Val Loss:   0.7596, Val Acc:   0.6872\n  [*] Saved best model.\nEpoch [3/20] for crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"                                                             ss=0.605]\r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.7240, Train Acc: 0.7138\n  Val Loss:   0.7454, Val Acc:   0.7110\n  [*] Saved best model.\nEpoch [4/20] for crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.6212, Train Acc: 0.7618\n  Val Loss:   0.5627, Val Acc:   0.7832\n  [*] Saved best model.\nEpoch [5/20] for crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.4623, Train Acc: 0.8278\n  Val Loss:   0.4999, Val Acc:   0.8034\n  [*] Saved best model.\nEpoch [6/20] for crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"Training:  86%|████████▌ | 424/495 [01:33<00:15,  4.55it/s, loss=0.583] IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.4205, Train Acc: 0.8431\n  Val Loss:   0.4828, Val Acc:   0.8201\n  [*] Saved best model.\nEpoch [7/20] for crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.3868, Train Acc: 0.8586\n  Val Loss:   0.4676, Val Acc:   0.8191\nEpoch [8/20] for crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.3584, Train Acc: 0.8701\n  Val Loss:   0.4549, Val Acc:   0.8292\n  [*] Saved best model.\nEpoch [9/20] for crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.3273, Train Acc: 0.8845\n  Val Loss:   0.4465, Val Acc:   0.8408\n  [*] Saved best model.\nEpoch [10/20] for crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2914, Train Acc: 0.9026\n  Val Loss:   0.4394, Val Acc:   0.8418\n  [*] Saved best model.\nEpoch [11/20] for crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2849, Train Acc: 0.9040\n  Val Loss:   0.4380, Val Acc:   0.8439\n  [*] Saved best model.\nEpoch [12/20] for crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2803, Train Acc: 0.9059\n  Val Loss:   0.4402, Val Acc:   0.8444\n  [*] Saved best model.\nEpoch [13/20] for crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2763, Train Acc: 0.9069\n  Val Loss:   0.4385, Val Acc:   0.8439\nEpoch [14/20] for crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2732, Train Acc: 0.9095\n  Val Loss:   0.4395, Val Acc:   0.8474\n  [*] Saved best model.\nEpoch [15/20] for crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2685, Train Acc: 0.9108\n  Val Loss:   0.4393, Val Acc:   0.8459\nEpoch [16/20] for crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2678, Train Acc: 0.9108\n  Val Loss:   0.4395, Val Acc:   0.8449\nEpoch [17/20] for crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2674, Train Acc: 0.9109\n  Val Loss:   0.4393, Val Acc:   0.8444\nEpoch [18/20] for crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2670, Train Acc: 0.9109\n  Val Loss:   0.4394, Val Acc:   0.8444\nEpoch [19/20] for crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2667, Train Acc: 0.9107\n  Val Loss:   0.4394, Val Acc:   0.8444\nEpoch [20/20] for crossvit_18_224\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_23/1389378619.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f\"best_{model_name}.pth\"))\n","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2661, Train Acc: 0.9112\n  Val Loss:   0.4394, Val Acc:   0.8444\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Evaluating crossvit_9_dagger_224\nEpoch [1/20] for crossvit_9_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.9785, Train Acc: 0.6018\n  Val Loss:   0.8117, Val Acc:   0.6948\n  [*] Saved best model.\nEpoch [2/20] for crossvit_9_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.7145, Train Acc: 0.7396\n  Val Loss:   0.8092, Val Acc:   0.6958\n  [*] Saved best model.\nEpoch [3/20] for crossvit_9_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.6261, Train Acc: 0.7775\n  Val Loss:   0.6022, Val Acc:   0.7883\n  [*] Saved best model.\nEpoch [4/20] for crossvit_9_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.5379, Train Acc: 0.8055\n  Val Loss:   0.6391, Val Acc:   0.7423\nEpoch [5/20] for crossvit_9_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.3864, Train Acc: 0.8604\n  Val Loss:   0.4295, Val Acc:   0.8423\n  [*] Saved best model.\nEpoch [6/20] for crossvit_9_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.3277, Train Acc: 0.8812\n  Val Loss:   0.3938, Val Acc:   0.8484\n  [*] Saved best model.\nEpoch [7/20] for crossvit_9_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2885, Train Acc: 0.9014\n  Val Loss:   0.3804, Val Acc:   0.8590\n  [*] Saved best model.\nEpoch [8/20] for crossvit_9_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2589, Train Acc: 0.9147\n  Val Loss:   0.3779, Val Acc:   0.8676\n  [*] Saved best model.\nEpoch [9/20] for crossvit_9_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2317, Train Acc: 0.9285\n  Val Loss:   0.3404, Val Acc:   0.8833\n  [*] Saved best model.\nEpoch [10/20] for crossvit_9_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2039, Train Acc: 0.9402\n  Val Loss:   0.3361, Val Acc:   0.8868\n  [*] Saved best model.\nEpoch [11/20] for crossvit_9_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1988, Train Acc: 0.9421\n  Val Loss:   0.3349, Val Acc:   0.8878\n  [*] Saved best model.\nEpoch [12/20] for crossvit_9_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1952, Train Acc: 0.9435\n  Val Loss:   0.3329, Val Acc:   0.8924\n  [*] Saved best model.\nEpoch [13/20] for crossvit_9_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1921, Train Acc: 0.9448\n  Val Loss:   0.3323, Val Acc:   0.8909\nEpoch [14/20] for crossvit_9_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1890, Train Acc: 0.9464\n  Val Loss:   0.3295, Val Acc:   0.8919\nEpoch [15/20] for crossvit_9_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1855, Train Acc: 0.9473\n  Val Loss:   0.3285, Val Acc:   0.8914\nEpoch [16/20] for crossvit_9_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1849, Train Acc: 0.9477\n  Val Loss:   0.3279, Val Acc:   0.8924\nEpoch [17/20] for crossvit_9_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1846, Train Acc: 0.9474\n  Val Loss:   0.3277, Val Acc:   0.8929\n  [*] Saved best model.\nEpoch [18/20] for crossvit_9_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1842, Train Acc: 0.9479\n  Val Loss:   0.3274, Val Acc:   0.8934\n  [*] Saved best model.\nEpoch [19/20] for crossvit_9_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1838, Train Acc: 0.9481\n  Val Loss:   0.3271, Val Acc:   0.8939\n  [*] Saved best model.\nEpoch [20/20] for crossvit_9_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_23/1389378619.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f\"best_{model_name}.pth\"))\n","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1834, Train Acc: 0.9482\n  Val Loss:   0.3270, Val Acc:   0.8934\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Evaluating crossvit_15_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/20] for crossvit_15_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 1.0124, Train Acc: 0.5772\n  Val Loss:   0.9070, Val Acc:   0.6609\n  [*] Saved best model.\nEpoch [2/20] for crossvit_15_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.7395, Train Acc: 0.7268\n  Val Loss:   0.6098, Val Acc:   0.7863\n  [*] Saved best model.\nEpoch [3/20] for crossvit_15_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.5992, Train Acc: 0.7727\n  Val Loss:   0.5540, Val Acc:   0.8004\n  [*] Saved best model.\nEpoch [4/20] for crossvit_15_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.4722, Train Acc: 0.8249\n  Val Loss:   0.4023, Val Acc:   0.8525\n  [*] Saved best model.\nEpoch [5/20] for crossvit_15_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.3057, Train Acc: 0.8886\n  Val Loss:   0.3501, Val Acc:   0.8747\n  [*] Saved best model.\nEpoch [6/20] for crossvit_15_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2688, Train Acc: 0.9055\n  Val Loss:   0.3350, Val Acc:   0.8807\n  [*] Saved best model.\nEpoch [7/20] for crossvit_15_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2401, Train Acc: 0.9147\n  Val Loss:   0.3093, Val Acc:   0.8919\n  [*] Saved best model.\nEpoch [8/20] for crossvit_15_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.2087, Train Acc: 0.9271\n  Val Loss:   0.2882, Val Acc:   0.8964\n  [*] Saved best model.\nEpoch [9/20] for crossvit_15_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1836, Train Acc: 0.9378\n  Val Loss:   0.2858, Val Acc:   0.9005\n  [*] Saved best model.\nEpoch [10/20] for crossvit_15_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1550, Train Acc: 0.9501\n  Val Loss:   0.2763, Val Acc:   0.9085\n  [*] Saved best model.\nEpoch [11/20] for crossvit_15_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1502, Train Acc: 0.9531\n  Val Loss:   0.2752, Val Acc:   0.9111\n  [*] Saved best model.\nEpoch [12/20] for crossvit_15_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1467, Train Acc: 0.9540\n  Val Loss:   0.2744, Val Acc:   0.9121\n  [*] Saved best model.\nEpoch [13/20] for crossvit_15_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1430, Train Acc: 0.9554\n  Val Loss:   0.2774, Val Acc:   0.9121\nEpoch [14/20] for crossvit_15_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1398, Train Acc: 0.9568\n  Val Loss:   0.2741, Val Acc:   0.9121\nEpoch [15/20] for crossvit_15_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1362, Train Acc: 0.9596\n  Val Loss:   0.2736, Val Acc:   0.9121\nEpoch [16/20] for crossvit_15_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1357, Train Acc: 0.9596\n  Val Loss:   0.2736, Val Acc:   0.9126\n  [*] Saved best model.\nEpoch [17/20] for crossvit_15_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 0.1353, Train Acc: 0.9593\n  Val Loss:   0.2735, Val Acc:   0.9126\nEpoch [18/20] for crossvit_15_dagger_224\n","output_type":"stream"},{"name":"stderr","text":"Training:   5%|▌         | 26/495 [00:04<01:15,  6.18it/s, loss=0.232] ","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Ensemble Majority Voting","metadata":{}},{"cell_type":"code","source":"# Ensemble Majority Voting\nensemble_predictions = np.array(ensemble_predictions)\nfinal_predictions = []\nfor i in range(len(ensemble_labels)):\n    final_predictions.append(np.bincount(ensemble_predictions[:, i]).argmax())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Calculating Metrics","metadata":{}},{"cell_type":"code","source":"# Calculate ensemble metrics\nensemble_accuracy = np.mean(np.array(final_predictions) == np.array(ensemble_labels))\nensemble_precision = precision_score(ensemble_labels, final_predictions, average='weighted')\nensemble_recall = recall_score(ensemble_labels, final_predictions, average='weighted')\nensemble_auroc = roc_auc_score(ensemble_labels, nn.functional.one_hot(torch.tensor(final_predictions), num_classes=num_classes), multi_class='ovr')\n#ensemble_aupr = average_precision_score(ensemble_labels, nn.functional.one_hot(torch.tensor(final_predictions), num_classes=num_classes), average='weighted')\n\n# Confusion Matrix for Ensemble\nensemble_cm = confusion_matrix(ensemble_labels, final_predictions)\nplt.figure(figsize=(8, 6))\nsns.heatmap(ensemble_cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(num_classes), yticklabels=range(num_classes))\nplt.title(\"Confusion Matrix for Ensemble\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.savefig(\"confusion_matrix_ensemble.png\")\nplt.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Saving Metrics","metadata":{}},{"cell_type":"code","source":"# Store Ensemble Metrics\nmetrics.append({\n    \"Model\": \"Ensemble\",\n    \"Test Accuracy\": ensemble_accuracy,\n    \"Precision\": ensemble_precision,\n    \"Recall\": ensemble_recall,\n    \"AUROC\": ensemble_auroc,\n})\n\n# Save Metrics to CSV\nmetrics_df = pd.DataFrame(metrics)\nmetrics_df.to_csv(\"model_metrics.csv\", index=False)\nprint(\"Metrics saved to model_metrics.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}